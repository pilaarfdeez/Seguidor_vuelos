name: Run Scraper

on:
  push:
    branches:
      - main  # Workflow wird bei jedem Push zum main Branch ausgelöst
  pull_request:
    branches:
      - main  # Workflow wird auch bei Pull Requests zum main Branch ausgelöst
  # schedule:
    # - cron: "0 12 * * *" # Run daily at 12:00 UTC

jobs:
  build:
    runs-on: ubuntu-latest  # Wir verwenden ein Ubuntu-Image

    steps:
      # Schritt 1: Checkout des Codes
      - name: Checkout repository
        uses: actions/checkout@v4

      # Schritt 2: Setze Python und installiere Abhängigkeiten
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      # Schritt 3: Installiere Abhängigkeiten
      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      # Schritt 4: Setze Google Chrome und ChromeDriver für Headless-Betrieb
      - name: Set up Chrome & ChromeDriver
        run: |
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
          google-chrome --version
          CHROME_VERSION=$(google-chrome --version | grep -oP '\d+\.\d+\.\d+' | head -1)
          echo "CHROME_VERSION: $CHROME_VERSION"
          CHROMEDRIVER_VERSION=$(curl -s "https://googlechromelabs.github.io/chrome-for-testing/latest-patch-versions-per-build.json" | jq -r ".builds[\"$CHROME_VERSION\"].version")
          echo "CHROMEDRIVER_VERSION: $CHROMEDRIVER_VERSION"
          wget -v -O chromedriver.zip "https://storage.googleapis.com/chrome-for-testing-public/$CHROMEDRIVER_VERSION/linux64/chromedriver-linux64.zip"
          ls -l chromedriver.zip
          unzip chromedriver.zip
          sudo mv chromedriver-linux64/chromedriver /usr/local/bin/chromedriver
          sudo chmod +x /usr/local/bin/chromedriver
          google-chrome --version
          chromedriver --version

        # Alternatively:
          # uses: browser-actions/setup-chrome@v3
          # with:
          #   chrome-version: 'latest'

      # Schritt 5: Füge Cookies aus GitHub Secrets hinzu
      - name: Write cookies to a file
        run: |
          mkdir -p data
          echo "${{ secrets.COOKIES_JSON }}" > data/cookies.json

      # Schritt 6: Führe das Scraper-Skript aus
      - name: Run scraper script
        run: |
          python main.py

      # Schritt 7: Speichere die JSON-Datei als Artefakt
      - name: Save the flight scraper output as Artifact
        uses: actions/upload-artifact@v4
        with:
          name: scraper-output
          path: output/flights.json
